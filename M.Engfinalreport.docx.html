<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_list_1-3{list-style-type:none}ol.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_2-6>li:before{content:"\0025cf  "}.lst-kix_list_2-7>li:before{content:"\0025cb  "}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-4>li:before{content:"\0025cb  "}.lst-kix_list_2-5>li:before{content:"\0025a0  "}.lst-kix_list_2-8>li:before{content:"\0025a0  "}ol.lst-kix_list_1-1{list-style-type:none}ol.lst-kix_list_1-2{list-style-type:none}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}.lst-kix_list_3-0>li:before{content:"\0025cf  "}.lst-kix_list_3-1>li:before{content:"\0025cb  "}.lst-kix_list_3-2>li:before{content:"\0025a0  "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}ul.lst-kix_list_3-1{list-style-type:none}.lst-kix_list_3-5>li:before{content:"\0025a0  "}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\0025cb  "}ul.lst-kix_list_3-0{list-style-type:none}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025cf  "}ul.lst-kix_list_3-5{list-style-type:none}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_3-8>li:before{content:"\0025a0  "}.lst-kix_list_3-6>li:before{content:"\0025cf  "}.lst-kix_list_3-7>li:before{content:"\0025cb  "}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ". "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,lower-latin) ". "}.lst-kix_list_1-2>li:before{content:"" counter(lst-ctn-kix_list_1-2,lower-roman) ". "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"" counter(lst-ctn-kix_list_1-3,decimal) ". "}.lst-kix_list_1-4>li:before{content:"" counter(lst-ctn-kix_list_1-4,lower-latin) ". "}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_list_1-7>li:before{content:"" counter(lst-ctn-kix_list_1-7,lower-latin) ". "}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:"" counter(lst-ctn-kix_list_1-5,lower-roman) ". "}.lst-kix_list_1-6>li:before{content:"" counter(lst-ctn-kix_list_1-6,decimal) ". "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_2-0>li:before{content:"\0025cf  "}.lst-kix_list_2-1>li:before{content:"\0025cb  "}.lst-kix_list_1-8>li:before{content:"" counter(lst-ctn-kix_list_1-8,lower-roman) ". "}.lst-kix_list_2-2>li:before{content:"\0025a0  "}.lst-kix_list_2-3>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c1{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:156pt;border-top-color:#000000;border-bottom-style:solid}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left;height:11pt}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center;height:11pt}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:center}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c13{padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c12{background-color:#ffffff;color:#0e101a;text-decoration:none;vertical-align:baseline;font-style:normal}.c24{background-color:#ffff00;color:#2d3b45;text-decoration:none;vertical-align:baseline;font-style:normal}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c26{color:#2d3b45;text-decoration:none;vertical-align:baseline;font-style:normal}.c28{border-spacing:0;border-collapse:collapse;margin-right:auto}.c14{padding-top:1.7pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c22{padding-top:16.3pt;padding-bottom:0pt;line-height:1.1015489101409912;text-align:left}.c3{padding-top:1.7pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{font-weight:700;font-size:11pt;font-family:"Arial"}.c11{font-weight:700;font-size:11pt;font-family:"Times New Roman"}.c15{font-weight:700;font-size:13pt;font-family:"Times New Roman"}.c25{font-weight:700;font-size:16pt;font-family:"Times New Roman"}.c20{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c10{font-weight:700;font-size:12pt;font-family:"Times New Roman"}.c4{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c27{font-weight:700;font-size:22pt;font-family:"Times New Roman"}.c21{margin-left:72pt;text-indent:36pt}.c19{padding:0;margin:0}.c17{height:0pt}.c8{height:11pt}.c18{padding-left:0pt}.c6{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c20 doc-content"><p class="c2"><span class="c5 c27">Domestic Helper Bot</span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c2"><span class="c5 c10">A Design Project Report</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">Present to the School of Electrical and Computer Engineering of Cornell University</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">In Partial Fulfillment of the Requirements for the Degree of</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">Master of Engineering, Electrical and Computer Engineering</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c7"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">Submitted by</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">Honglei Huo, Zhiming Xie</span></p><p class="c0"><span class="c5 c10"></span></p><p class="c2"><span class="c5 c10">MEng Field Advisor: Joseph F. Skovira</span></p><p class="c0"><span class="c5 c11"></span></p><p class="c2"><span class="c5 c10">Degree Date: January 2023</span></p><p class="c0"><span class="c5 c23"></span></p><p class="c2"><span class="c5 c25">Abstract</span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c2"><span class="c5 c15">Master of Engineering Program</span></p><p class="c0"><span class="c5 c15"></span></p><p class="c2"><span class="c5 c15">School of Electrical and Computer Engineering</span></p><p class="c0"><span class="c5 c15"></span></p><p class="c2"><span class="c5 c15">Cornell University</span></p><p class="c0"><span class="c5 c15"></span></p><p class="c2"><span class="c5 c15">Design Project Report</span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c0"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c7"><span class="c5 c11"></span></p><p class="c16"><span class="c10">Project Title: </span><span class="c5 c4">Domestic Helper Bot</span></p><p class="c7"><span class="c5 c10"></span></p><p class="c7"><span class="c5 c10"></span></p><p class="c16"><span class="c10">Author: </span><span class="c5 c4">Honglei Huo, Zhiming Xie</span></p><p class="c7"><span class="c5 c10"></span></p><p class="c7"><span class="c5 c10"></span></p><p class="c16"><span class="c10">Abstract: </span><span class="c12 c4">In response to the COVID-19 pandemic, many individuals have transitioned to remote work arrangements, resulting in increased time spent on home renovation projects. Consequently, our team has developed the Domestic Helper Bot (DHB) as a versatile assistant to address the needs of modern homeowners. The primary objective of the DHB is to facilitate efficient robot-to-human handover tasks within a domestic environment.</span></p><p class="c7"><span class="c12 c4"></span></p><p class="c16"><span class="c12 c4">The DHB system comprises a Roomba base, equipped with a sophisticated robotic arm featuring an integrated camera, an additional camera mounted on the Roomba robot, and a USB computer speaker for audio output. The entire system operates on Python 3 programming language. The DHB is designed to respond to voice commands, accurately recognize the owner&#39;s voice, identify the location of the requested tool, retrieve the tool utilizing its robotic arm, and subsequently deliver the tool to the owner in a timely manner. This innovative project holds significant potential to enhance the quality of life by streamlining household tasks and fostering greater convenience for homeowners.</span></p><p class="c7"><span class="c12 c4"></span></p><p class="c22"><span class="c12 c10">Introduction</span></p><p class="c22"><span class="c12 c4">&nbsp; &nbsp; &nbsp;The present project integrates three core components&mdash;Vision, Inverse Kinematics, and Voice Recognition&mdash;to create a seamless user experience. Our team employs the SpeechRecognition package to accurately discern user commands delivered via a USB microphone. Upon interpreting the command, the mobile robot navigates to a predetermined toolbox waypoint. As the robot approaches the vicinity of the toolbox, it momentarily halts and initiates the computer vision algorithm, utilizing the camera integrated into the robotic arm to identify the desired tool. This process primarily involves OpenCV-based color recognition methods in tandem with the inverse kinematics of the robotic arm. The primary advantage of implementing color recognition lies in its capacity for instantaneous tool identification without necessitating extensive Neural Network training, which would entail significantly greater computational demands.</span></p><p class="c22"><img></p><p class="c3"><span class="c12 c10">System Requirements</span></p><p class="c3 c8"><span class="c12 c4"></span></p><p class="c3"><span class="c5 c10">A. Roomba Assembly</span></p><p class="c3"><span class="c5 c4">As depicted in the accompanying figure, the Roomba Create2 serves as the foundational platform for the entire system. Three additional laser-cut layers have been incorporated to support the various components of the system. The top layer is designed to accommodate the robotic arm and camera, while the second layer houses the power system. The bottom layer functions to secure the other layers in place without causing any damage to the Roomba.</span></p><p class="c3 c8"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c10">B. Device Interconnectivity</span></p><p class="c3"><span class="c5 c4">The Raspberry Pi 4B is securely mounted onto the expansion board and employs a ribbon cable to enable efficient power and data transmission. The robotic arm is also integrated with the expansion board through a wired connection. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure1. Roomba Assembly</span></p><p class="c3"><span class="c5 c4">Consequently, providing power to the expansion board is adequate for operating the entire robotic arm system, thereby negating the necessity for auxiliary energy resources for the Raspberry Pi and the robotic arm. The camera affixed to the robotic arm, the supplementary camera, and the microphone utilized for voice recognition are all interconnected with the Raspberry Pi via USB ports, ensuring seamless communication and functionality.</span></p><p class="c3"><span class="c5 c10">Achievements and Current Progress</span></p><p class="c3 c8"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c4">1. Computer Vision Algorithm</span></p><p class="c3 c8"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">The computer vision algorithm is the core control algorithm of the entire robot project. We use the functions from OpenCV to select a specific color, and then combine it with the robot&#39;s control algorithm so that Roomba can find and move to the position of the toolbox, and the robot arm can recognize and pick up corresponding items.</span></p><p class="c16"><span class="c5 c4">The core idea of this algorithm is to first convert the picture captured by the camera from the BGR color space to the HSV color space, so that the computer can process the color more accurately. Next, find the part of the image between the upper and lower thresholds of the specified color (that is, the part we need). The third step is to erode the converted image to remove excess noise and avoid the algorithm being affected by noise. To erode, we choose 3x3 kernels and 2 iterations. Because these two parameters can reduce the noise without causing major changes to the image.</span></p><p class="c7"><span class="c5 c4"></span></p><a id="t.70fc7fb2425e61dcadaea4ed606a79c7e0ef6c44"></a><a id="t.0"></a><table class="c28"><tr class="c17"><td class="c1" colspan="1" rowspan="1"><p class="c9"><span class="c5 c4">Frame captured by camera</span></p></td><td class="c1" colspan="1" rowspan="1"><p class="c9"><span class="c5 c4">Image only remain the green object</span></p></td><td class="c1" colspan="1" rowspan="1"><p class="c9"><span class="c5 c4">Image after erode</span></p></td></tr><tr class="c17"><td class="c1" colspan="1" rowspan="1"><p class="c9"><img></p></td><td class="c1" colspan="1" rowspan="1"><p class="c9"><img></p></td><td class="c1" colspan="1" rowspan="1"><p class="c9"><img></p></td></tr></table><p class="c2"><span class="c5 c4">Table 1. Image Processing</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">2. Robotic Arm Control Algorithm</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">For the control part of the robot arm, we use the output of the computer vision algorithm as the input of the robot arm control algorithm, and divide the picture captured by the camera into three regions.</span></p><p class="c16"><span class="c5 c4">As Figure 1 shown, the size of the frame is 640x480 pixels, since we need to make sure the center of the target object is in the middle of the camera screen, so as to ensure that the grabbing direction of the robotic arm and the target object are on the same straight line. Because the Raspberry Pi and the operating system we use cannot achieve the performance of a real-time operating system, we cannot make the robotic arm be perfectly aligned with the centerline of the object. So we draw a range at a distance of 16 pixels from both sides of the centerline of the screen. If the center of the target object appears to the left of Xmin or the right of Xmax, the control algorithm will use the servo to rotate the arm horizontally until the center of the object is between Xmin and Xmax. Additionally, in order to avoid mistakes, the algorithm will only be executed correctly if the selected color area is greater than the preset threshold, and any results smaller than this threshold will be ignored. In this case, the orientation of the robotic arm is guaranteed to be correct and the grasping operation can begin.</span></p><p class="c2"><img></p><p class="c2"><span class="c5 c4">Figure 2. Schematic diagram of the logic of the robotic arm control algorithm</span></p><p class="c0"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">3. Roomba Control Algorithm</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">camThread class is built and implemented to allow the raspberry pi to control two cameras in different channels. Port 0 connects with the robotic arm camera and port 2 connects with the Roomba camera which is mounted on the top front platform. The ports are interchangeable, and the algorithm can activate a specific camera when the command is passed through from the terminal/voice command. The Roomba camera uses the same algorithm as the robotics arm algorithm but with a lower x bias, which is 100. This bias value is obtained from the experimental result when a red shoebox was used as a toolbox. By adding this bias value to the current x coordinate value, which is given by the function cv2.minEnclosingCircle(area), the Roomba camera can locate the x-center of the toolbox more conveniently. Having the bias can avoid adjusting the Roomba toward the left or right of the toolbox center frequently. As a result, the Roomba can seemingly travel in a straight line when the Roomba camera sees the toolbox from a distance. In the experiment, we placed the Roomba 3 to 4 feet away from the toolbox. (Figure 3) &nbsp;</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c2"><img></p><p class="c16 c21"><span class="c5 c4">Figure 3: Roomba Traveling Velocity Graph</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c4 c5">In addition to adding the bias value, a value of 20 is added to the left wheel velocity of the Roomba to balance the uncoordinated speed between the left and right wheels.(Figure 4) To have the Roomba stop right at the toolbox, we include two safety features in the algorithm. First of all, when the Roomba camera is activated, it will obtain the real-time value of the toolbox&#39;s radius, given by the function cv2.minEnclosingCircle(area). (Figure 3) When the radius is in the range between 20 and 100, the Roomba travels with a speed of 100 rev/min, and when the radius is larger than 100, the Roomba will slow down to 40 rev/min, and when the radius is larger than 200, the Roomba will travel at a speed to 15 rev/min for 2 seconds and stop in front of the toolbox. The stopping distance is about 2 centimeters away. When the Roomba has arrived at the toolbox, the algorithm will switch the camera port to activate the robotics arm camera and commands the arm to recognize the target object and obtain the object. </span></p><p class="c7"><span class="c5 c4"></span></p><p class="c2"><img></p><p class="c2"><span class="c5 c4">Figure 4: Toolbox drawing with xmin, xmax, and bias &nbsp;</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">4. Voice Recognition</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">Speech_recognition API is implemented in the algorithm to recognize basic commands such as toolbox and color names. Each tool corresponds to a unique color, and the user only needs to input the color name, and the robot will grab the correct item. The user will need to speak directly to a USB microphone that is connected to the raspberry pi. The voice command will serve as an input source(audio) and will be passed to the function r.recognize_google(audio). After the voice is recognized and outputted as a word, the algorithm will check if such a word exists in the pre-defined vocab library, which is an array of valid commands. If the word matches one of the strings in the array, then the algorithm will pass the word as input to the corresponding function and activate the robot arm or Roomba to perform certain actions. &nbsp;</span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><img></p><p class="c7"><span class="c5 c4"></span></p><p class="c7"><span class="c5 c4"></span></p><p class="c7"><span class="c5 c4"></span></p><p class="c7"><span class="c5 c4"></span></p><p class="c7"><span class="c5 c4"></span></p><p class="c16"><span class="c5 c4">5. Main Program Logic</span></p><p class="c16"><span class="c5 c4">First, we define all of the above algorithms as functions with user input as connections. To form a complete loop to ensure that each function can be executed in the preset order.</span></p><p class="c3 c8"><span class="c5 c4"></span></p><p class="c3"><span class="c10">Results</span></p><p class="c3"><span class="c4 c26">The results of the project are basically the same as our expectations, the robot can travel to the target location correctly and slow down at the appropriate location to avoid severe collisions. The robotic arm can also start to recognize the target object at the right time and rotate to the correct angle. However, there are also some shortcomings. For example, the processing speed and accuracy of the speech recognition function are not as good as expected. Because the grabbing distance of the robotic arm is fixed, sometimes the grabbing fails.</span></p><p class="c3 c8"><span class="c5 c10"></span></p><p class="c3"><span class="c5 c10">Future Work</span></p><ol class="c19 lst-kix_list_1-0 start" start="1"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Computer Vision Algorithm</span></li></ol><p class="c3 c6"><span class="c5 c4">Since we are using colors currently, we have to prepare many different colored blocks to represent different tools. When we developed the CV algorithm, we also did some tests on using machine learning to recognize different tools based on Tensorflow lit and YOLO. </span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><p class="c14 c6"><img></p><p class="c14 c6"><span class="c5 c4">Figure 6.Identify the hammer shown on the screen</span></p><p class="c14 c8 c6"><span class="c5 c4"></span></p><p class="c3 c6"><span class="c5 c4">We also explored another way to identify different tools, which is using Apriltags. </span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><p class="c14 c6"><img></p><p class="c6 c14"><span class="c5 c4">Figure 7.Identify different Apriltags in the same family</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><p class="c3 c6"><span class="c5 c4">After our tests, these two methods are achievable, and will be more accurate and convenient than the current algorithm.</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><ol class="c19 lst-kix_list_1-0" start="2"><li class="c3 c6 c18 li-bullet-0"><span class="c4">Robotic Arm Control Algorithm</span></li></ol><p class="c3 c6"><span class="c5 c4">At present, the robotic arm can correctly identify the direction of the target object and rotate to align with the center of the target. However, it can only grasp objects at a relatively fixed distance. If the distance between the object and the robotic arm is too far or too close, the robotic arm will fail to grasp.</span></p><p class="c3 c6"><span class="c5 c4">For this part, the solution we thought of was to install a top-down camera above the toolbox. The camera can transmit the position of the recognized tool back to the Raspberry Pi through the wireless network. The target object can be successfully grasped by the inverse kinematics robotic arm, which is no longer limited by the distance of the object.</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><ol class="c19 lst-kix_list_1-0" start="3"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Voice Recognition</span></li></ol><p class="c3 c6"><span class="c5 c4">Our speech recognition function currently has two major problems, one is slow processing speed, and the other is that it is easily affected by environmental noise. We think ambient noise may also be an important reason for the slow processing speed. The method we think of is that the continuous noise in the environment can be reduced through the algorithm,such as a filter, and the confidence level can be adjusted to judge whether it is noise.</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><ol class="c19 lst-kix_list_1-0" start="4"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Roomba control Algorithm</span></li></ol><p class="c3 c6"><span class="c5 c4">The navigation method we currently use is based on computer vision algorithms, but the disadvantage of this navigation method is that once the robot cannot see the target position within the field of vision or encounters obstacles during the travel process, it is difficult for the robot to move correctly to the target position. In future development, there are two potential solutions. One is to further develop the computer vision algorithm so that it can detect obstacles and guide the robot to avoid them, and the other is to use laser radar or distance sensors to avoid direct collisions between robot and obstacles.</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><ol class="c19 lst-kix_list_1-0" start="5"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Power Supply</span></li></ol><p class="c3 c6"><span class="c5 c4">All testing at this stage has been done with a power cord connecting the robot to wall power. Therefore, the movement area of the robot is greatly limited. Because Roomba has a built-in battery, we only need to find a 12V 5A mobile power supply to connect to the robot arm system to get rid of the limitation of the length of the power cord.</span></p><p class="c3 c8 c6"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c10">Individual Works</span></p><p class="c3 c8"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c4">Honglei Huo:</span></p><ul class="c19 lst-kix_list_3-0 start"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Developed computer vision algorithms</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Combining computer vision algorithms and robotic arms for target detection</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Main program logic integration</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Modification and assembly of robot parts.</span></li></ul><p class="c3 c8"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c4">Zhiming Xie: </span></p><ul class="c19 lst-kix_list_2-0 start"><li class="c3 c18 c6 li-bullet-0"><span class="c4">Developed robotic arm control algorithms</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Servos Control Algorithm of Robotic Arm for Target Grasping</span></li><li class="c6 c13 li-bullet-0"><span class="c4">Voice recognition and roomba algorithm</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Main program logic coding</span></li><li class="c13 c6 li-bullet-0"><span class="c4">Use laser cutting to make the structural parts needed for the robot.</span></li></ul></body></html>